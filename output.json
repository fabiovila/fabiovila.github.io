{
    "url" : ["/eletronica/2023/12/29/geradores-e-divisores-de-oitavas-org%C3%A3os-antigos.html","/eletronica/2023/11/15/siglas-de-sensores-obd2.html","/aplicadas/2023/01/10/problema-dual-primal.html","/python/2023/01/02/usando-servicos-openai.html","/aplicadas/2022/10/20/cvpy-knapsack.html","/aplicadas/2022/08/02/probabilidade-classica-frequentista-e-subjetiva.html","/aplicadas/2022/08/01/infer%C3%AAncia-estatistica.html","/eletr%C3%B4nica/2021/02/10/simulacao-circuito-RL-python.html","/eletr%C3%B4nica/2021/01/10/filtro-fir.html","/ci%C3%AAncias/2020/06/10/indu%C3%A7%C3%A3o-falseabilidade.html","/ci%C3%AAncias/2020/06/10/o-t%C3%A9cnico-que-chutou-a-maquina-copy.html"],
    "title" : ["Órgãos Antigos dos Anos 70: Geradores e Divisores de Oitavas - Música Eletrônica","Siglas de sensores e atuadores OBD2 - Eletrônica automotiva","O problema dual e primal - Otimização Matemática","Usando os serviços OpenAI para criar um ChatBot - Python","Usando CVXPY para o problema da mochila (knapsack) - Otimização","Probabilidade clássica frequentista e subjetiva - Matemática","Inferência estatística - Matemática","Usando Python para resolver ODEs de circuitos elétricos: RL - Python","Filtros FIR - Processamento de sinais","Indução e Falseabilidade - Filosofia da Ciência","O técnico que chutou a máquina - Filosofia da Ciência"],
    "content" : "[\"Órgãos Antigos dos Anos 70: Geradores e Divisores de Oitavas\\n\\nOs anos 70 foram uma época de efervescência musical e inovação tecnológica, e os órgãos eletrônicos desempenharam um papel crucial nesse cenário. Esses instrumentos, muitas vezes utilizados em bandas de rock progressivo, jazz fusion e música eletrônica da época, eram dotados de características sonoras únicas, inicialmente construidos com o objetivo de serem fiéis aos órgãos de tubos, sua sonoridade única e portabilidade logo achou seu lugar próprio na música.\\n\\nGeradores de Oitavas: A Base do Som Único\\n\\nOs geradores de oitavas são circuitos eletrônicos projetados para gerar sinais de áudio em diferentes oitavas. No contexto desses instrumentos, os geradores de oitavas eram responsáveis por criar as frequências fundamentais e suas oitavas superiores e inferiores.\\n\\nOs órgãos antigos, como o lendário Hammond B3, utilizavam geradores de oitavas para produzir um som encorpado e cheio. A manipulação desses geradores nos Drawbars permitia aos músicos criar timbres ricos e expressivos, tornando esses instrumentos essenciais para a música da década.\\n\\nDivisores de Oitavas: Enfatizando a Profundidade Sonora\\n\\nEm adição aos geradores de oitavas, os órgãos antigos dos anos 70 também contavam com divisores de oitavas. Esses dispositivos eram responsáveis por dividir o sinal gerado pelos osciladores em diferentes oitavas, permitindo uma abordagem mais versátil e dinâmica na criação de sons.\\n\\nOs divisores de oitavas eram particularmente notáveis pela forma como enriqueciam a paleta sonora dos músicos. Ao controlar a distribuição do sinal em oitavas específicas, os músicos podiam criar texturas sonoras complexas, empregando escalas e acordes de forma única. Isso proporcionava uma variedade de opções tonais, desde as profundezas ressonantes das oitavas mais baixas até as agudas e cristalinas oitavas superiores.\\n\\nO par de chips S50240 e S10430\\n\\nAntes do advento de circuitos integrados especializados em geração e divisão de oitavas o processo de dividir as frequências das notas eram feitos com flip-flops discretos ou integrados. Os circuitos integrados S50240 e S10430 facilitaram a construção de órgãos eletrônicos polifônicos com o uso de apenas 2 ou três chips especializados.\\nO S50240 é o circuito integrado responsável por sintetizar as oitavas superiores do instrumento. Um oscilador tipicamente de 2Mhz será ligado na entrada e dividido em 12 frequências gerando 12 ondas quadradas com 50% de duty cycle correspondente a cada nota musical.\\nAs divisões podem ser conferidas na tabela abaixo:\\n\\n\\n  \\n    \\n      Divisão\\n      Frequência resultante\\n      Nota\\n    \\n  \\n  \\n    \\n      ÷478\\n      4184.10 Hz\\n      Dó inferior\\n    \\n    \\n      ÷451\\n      4434.60 Hz\\n      Dó sustenido\\n    \\n    \\n      ÷426\\n      4694.80 Hz\\n      Ré\\n    \\n    \\n      ÷402\\n      4975.10 Hz\\n      Ré sustenido\\n    \\n    \\n      ÷379\\n      5277.80 Hz\\n      Mi\\n    \\n    \\n      ÷358\\n      5586.60 Hz\\n      Fá\\n    \\n    \\n      ÷338\\n      5917.20 Hz\\n      Fá sustenido\\n    \\n    \\n      ÷319\\n      6269.60 Hz\\n      Sol\\n    \\n    \\n      ÷301\\n      6644.50 Hz\\n      Sol sustenido\\n    \\n    \\n      ÷284\\n      7042.30 Hz\\n      Lá\\n    \\n    \\n      ÷268\\n      7462.70 Hz\\n      Lá sustenido\\n    \\n    \\n      ÷253\\n      7905.10 Hz\\n      Si\\n    \\n    \\n      ÷239\\n      8362.20 Hz\\n      Dó superior\\n    \\n  \\n\\n\\nO S50240 sozinho nos permite apenas gerar 12 notas fundamentais. Para se obter as notas das oitavas inferiores dois S10430 são usados em conjunto com o S50240. Cada S10430 dividirá 4 notas em 4 oitavas e 2 notas em 3 oitavas totalizando por chip 22 teclas por chip e para se obter 44 teclas dois S10430 são usados em cascata. Cada divisor tem 4 saídas diferentes permitindo que com um potênciometro o músico possa misturar 4 oitavas da mesma nota nos níveis desejados alterando assim o timbre da nota. Além das 4 saídas as entradas das teclas no S10430 são sensíveis a tensão aplicada, permitindo a criação de efeitos de ataques e liberação das teclas, normalmente utilizando circuitos RC em cada tecla.\\n\\nO diagrama abaixo retirado do datasheet do S10430 ilustra facilmente essas ligações.\\n\\n\\n\\nOutros chips\\n\\nEsses chips são obsoletos e difíceis de se achar no mercado, porém não são os únicos a fazerem essa funções. Outras empresas lançaram no mercado outros chips muito conhecidos:\\n\\n13 Note Top Octave Synthetizer (TOS)\\n50% Duty Cycle\\n\\n\\n  MO83B\\n  M50240\\n  M5891B1\\n  S50240\\n  50240\\n\\n\\n\\n\\n13 Note Top Octave Synthesizer (TOS)\\n30% Duty Cycle\\n\\n\\n  MO82B\\n  M50241\\n  M5891A\\n  S50241\\n  50241\\n\\n\\n\\nQual a importância do Duty Cycle?\\nTer um duty cycle de 30% ou 50% influencia nos harmônicos gerados pela onda quadrada. O duty cycle de 50% gera harmônicos impares que para alguns gostos é mais ácido e agudo. Enquanto usar um duty cycle de 30% gera um forte harmônico par resultando em um timbre mais leve e macio. Alguns construtores alegam que um duty cycle de 25% é o melhor valor para gerar harmônicos pares.\\n\\n\\n\\n\\n12 Note Top Octave Synthesizer (TOS)\\n\\n\\n  MO86B\\n  M50242\\n  S50242\\n  50242\\n  AY-1-0212\\n  ECG-2043\\n\\n\\n\\n\\n12 Note Top Octave Synthesizer (TOS)\\n\\n\\n  MO87B\\n  MC86B1\\n  ECG-2043\\n\\n\\nOpções de substituição são poucas. O técnico dependerá de peças velhas. Alguns podem fazer seu próprio TOS utilizando FPGA ou algum microcontrolador rápido o suficiente para gerar 12 ondas quadradas sem desvio de fase. É importante se atentar que a maioria dos órgãos tem Vibrato quando se varia levemente a frequência do oscilador VCO de 2MHz.\\n\\n\\n\\nFontes:\\n\\nhttp://www.armory.com\\n\\nS50240 e S10430 Datasheets\\n\\nWikipedia\\n\", \"Se você tem um leitor OBD2 em inglês essa lista pode te ajudar a desvendar as siglas na leitura.\\n\\n\\n  \\n    \\n      Informação exibida\\n      Explicação\\n    \\n  \\n  \\n    \\n      Fuel System 1 Status [Status 1 ou Fuelsys1]\\n      Em sistema de controle OL - Open Loop significa que o controle está sendo feito as ‘cegas’, sem o uso dos sensores. Em modo CL - Closed Loop o controle está sendo feito utilizando os sensores. A sonda lambda é o principal sensor utilizado para controlar a mistura ar-combustível e para funcionar ela precisa estar quente.Ao ligar o carro com o motor frio a sonda lambda (fria) não será utilizada de início, então a ECU operará o motor em modo OL - Open Loop até que a sonda lambda envie os dados corretos quando finalmente a ECU (agora com valores corretos dos sensores já aquecidos) muda para Closed Loop.\\n    \\n    \\n      Calculated Load Value [CLV or Load_PCT]\\n      Carga do motor.\\n    \\n    \\n      Engine Coolant Temp [ECT]\\n      Temperatura do liquido de arrefecimento (água do radiador).\\n    \\n    \\n      Short Term Fuel Trim-Bank 1 [STFT 1 ou SHRTFT1]\\n      Ajuste imediato da mistura estequiometria. Se o valor é positivo a ECU está adicionando combustível (mistura rica) se negativo a ECU está retirando combustível (mistura pobre). Esse ajuste se perde ao desligar o carro. Quando se está ‘descendo na banguela’, por exemplo, com o carro engatado a ECU para de mandar combustível mostrando valores negativos para STFT.\\n    \\n    \\n      Long Term Fuel Trim- Bank 1 [LTFT 1 ou LONGFT1]\\n      Ajuste de longo prazo da mistura com base no histórico do sensor explicado acima. Se por exemplo o sensor imediato ficar por muito tempo em 6% o Long Term Fuel Trim irá incrementar seu ajuste para compensar até que o Short Term fique em 0%. Esse ajuste não se perde ao desligar o carro, é um dos indicadores mais importantes de falhas no motor.\\n    \\n    \\n      Engine RPM [RPM]\\n      O RPM do motor\\n    \\n    \\n      Vehicle Speed Sensor [VSS]\\n      Velocidade do veiculo\\n    \\n    \\n      Ignition Timing Advance #1 [Sparkadv]\\n      Mostra o ângulo de ignição ou mais conhecido como “ponto de ignição”. Diminui com o aumento da temperatura.\\n    \\n    \\n      Intake Air Temp [IAT]\\n      Temperatura do ar.\\n    \\n    \\n      Air Flow Rate from Mass Air Flow Sensor [MAF]\\n      A quantidade de ar entrando no sistema de admissão. Em geral está associado ao sensor MAF.\\n    \\n    \\n      Absolute Throttle Position [TP or ABS_TP]\\n      Posição do sensor de corpo de borboleta. Ao pisar no pedal do acelerador esse sinal deve subir.\\n    \\n    \\n      Bank 1 – Sensor 2 Volts [O2S12]\\n      Tensão da sonda lambda pós catalisador. Em situações normais não deve variar.\\n    \\n    \\n      Bank 1 – Sensor 1 [O2S11]\\n      Tensão da sonda lambda antes do catalisador. No osciloscópio se apresenta como uma onda oscilante entre 900mV e 200mV. Valores acima de 450mV indica mistura rica e valores abaixo mistura pobre. O sinal não deve ser contínuo, mas oscilante. O ideal é oscilar simetricamente em mistura pobre e rica, se ficar por mais tempo acima ou abaixo de 450mV possivelmente é sinal de problemas.\\n    \\n    \\n      OBD Requirements OBD and OBD2 [OBDSUP]\\n      O protocolo de comunicação utilizado pelo OBD\\n    \\n  \\n\\n\", \"Os problemas primal e dual estão intrinsecamente ligados na teoria da dualidade em programação linear. Eles representam duas formulações diferentes de um mesmo problema de otimização linear, sendo inter-relacionados de maneira específica.\\n\\nProblema Primal\\n\\nO problema primal é a formulação original do problema de otimização linear. Geralmente, é expresso na forma padrão:\\n\\n\\\\[\\\\text{Maximize } \\\\quad c^Tx\\\\]\\n\\\\[\\\\text{sujeito a } \\\\quad Ax \\\\leq b\\\\]\\n\\\\[x \\\\geq 0\\\\]\\n\\nonde:\\n\\n  $c$ é um vetor de coeficientes da função objetivo,\\n  $x$ é o vetor de variáveis de decisão,\\n  $A$ é uma matriz de coeficientes das restrições,\\n  $b$ é o vetor de limites das restrições.\\n\\n\\nA meta é maximizar ou minimizar a função objetivo $c^Tx$ sujeita a um conjunto de restrições lineares.\\n\\nProblema Dual\\n\\nO problema dual é uma formulação alternativa derivada do problema primal usando a teoria de dualidade. Introduz-se multiplicadores de Lagrange para as restrições do problema primal, gerando assim o problema dual. A formulação geral do problema dual é:\\n\\n\\\\[\\\\text{Minimize } \\\\quad b^Ty\\\\]\\n\\\\[\\\\text{sujeito a } \\\\quad A^Ty \\\\geq c\\\\]\\n\\\\[y \\\\geq 0\\\\]\\n\\nonde:\\n\\n  $y$ é o vetor de multiplicadores de Lagrange (ou variáveis duais),\\n  $b$ é o vetor de termos constantes das restrições do primal,\\n  $A^T$ é a matriz transposta de $A$,\\n  $c$ é o vetor de coeficientes da função objetivo do primal.\\n\\n\\nO objetivo do problema dual é encontrar os multiplicadores de Lagrange que minimizam $b^Ty$ sujeito às restrições lineares.\\n\\nRelação de Dualidade\\n\\nA relação fundamental entre os problemas primal e dual é expressa pelo Teorema da Dualidade, que estabelece que o valor ótimo do problema primal é sempre maior ou igual ao valor ótimo do problema dual, e vice-versa. Matematicamente, isso é expresso como:\\n\\n\\\\[\\\\text{Para todo } x \\\\text{ factível no primal e para todo } y \\\\text{ factível no dual, temos } c^Tx \\\\geq b^Ty\\\\]\\n\\nSe ambos os problemas alcançam seus valores ótimos, então há igualdade entre eles. Esta relação é conhecida como dualidade forte.\\n\\nA dualidade não apenas fornece uma maneira de verificar a solução ótima, mas também oferece informações valiosas sobre a sensibilidade do problema às mudanças nos parâmetros e fornece soluções alternativas.\\n\", \"Em 2023 Taylor Swift foi eleita a personalidade do ano, mas há quem diga que 2023 foi mesmo o ano da Inteligência Artificial. E de fato há uma corrida das empresas que ficaram para trás do lançamento do modelo  text-davinci da OpenAI que era um modelo rudimentar comparado aos gpt-turbo-3.5 e o mais moderno gpt-4 usados hoje, mas fez barulho. O Google lançou o Bard mas não impressionou. A Baidu lançou seu modelo porém restrito ainda a sua área, enquanto a Amazon e a Apple correm atrás do prejuízo. Elon Musk lançou o Grok1, mas até o momento tinha fila de espera para usar. A Apple tenta negociar um acordo milionário para poder usar os artigos da imprensa americana para seu modelo de linguagem2. A Microsoft foi astuta e financiou a OpenAI quando percebeu o potencial do lançamento do GPT-2 e hoje oferece esses modernos modelos como serviço pago ou gratuito.\\n\\nFazendo um chatbot usando o OpenAI\\n\\nFazer um chatbot usando os serviços da OpenAI é simples, usa-se somente UMA chamada de função do módulo da openai em python.\\nCadastro e criação da Secret Key\\nPrimeiro é preciso fazer o cadastro na plataforma de serviço OpenAI API no site https://platform.openai.com/signup. Até o momento eles oferecem créditos para avaliação do serviço e não é preciso cartão de crédito para usar esses créditos. Entretanto a avaliação fica restrista ao modelo gpt-3.5-turbo que não deixa de ser um modelo capaz e poderoso. Se você usa a versão gratuita do site você usa esse modelo.\\n\\nFeito o cadastro é preciso gerar a OpenAI KEY para que se possa usar os serviços.\\n\\n\\n\\nClique em “Create new Secret Key” para criar sua Secret Key, como visto na figura acima.\\n\\nInstalação do módulo de acesso\\nEm seguida é necessário instalar o módulo da openai. Isso pode ser feito pelo PIP ou preferencialmente pelo seu gerenciador de pacotes em sua distribuição Linux.\\npip install openai\\n\\nFazendo sua primeira conversa\\n\\nFinalmente podemos fazer a primeira chamada para a OpenAI. Primeiramente é preciso criar uma instância OpenAI usando sua SECRET_KEY do passo anterior. Para fazer uma requisição usa-se o chat.completions.create como visto no código abaixo:\\n\\n\\n# Importa o módulo OpenAI instalado com pip anteriormente\\nfrom openai import OpenAI\\n# Cria uma instância OpenAI. Utilize aqui sua Secret Key no lugar de SUA_SECRET_KEY ( entre aspas )\\nclient = OpenAI(SUA_SECRET_KEY)\\n\\n# Finalmente fazemos uma requisição ao serviço de chat\\ncompletion = client.chat.completions.create(\\nmodel=\\\"gpt-3.5-turbo\\\",  # Modelo usado.\\n# A personalidade e o texto enviado ao chat.\\nmessages=[\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"Você é um assistente virtual divertido e bem humorado.\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Qual o sentido da vida?\\\"}\\n]\\n)\\n\\n# E finalmente exibimos a resposta do modelo \\\"gpt-3.5-turbo\\\" a questão \\\"Qual o sentido da vida?\\\"\\nprint(completion.choices[0].message)\\n\\n\\nPersonalizando e embutindo informações\\nBasicamente é só isso. Configura a Key e chama os serviço de complementação de texto completions.create com a personalidade e a mensagem. Fácil não?!\\n\\nAgora é uma boa hora para explicar os roles da requisição. Como se pode ver no código temos uma list com duas dict com as chaves “role” e “content”. Essas informações serão enviadas ao modelo para configurá-lo com a personalidade e informações que se deseja.\\n\\nRole System\\n\\nO role system é o mais forte e determina a personalidade do assistente. Veja alguns exemplos de role system:\\n\\n  “Você é uma IA da empresa Octopus S.A.. A Empresa está localizada no endereço X e tem o telefone Y.”\\n  “Você é um bot do telegram divertido e bem humorado. Sua personalidade é ácida e irônica”\\n  “Você é um assistente virtual no twitch do Gamer Lucão, você adora Minecraft e Playstation 5”\\n\\n\\nEsse role é opcional. Não é preciso enviar nada se não quiser.\\n\\nRole Assistant\\n\\nO role assistant é útil para enviar a conversa anterior e informações relevantes. Em geral usa-se formatação de strings no python para embutir informações e o texto anterior. Veja alguns exemplos:\\n\\nEXPEDIENTE = \\\"estamos abertos até as 18:00hrs\\\" \\n...\\n{\\\"role\\\": \\\"system\\\", \\\"content\\\": F\\\"Hoje é {datetime.datetime.now()} e hoje {EXPEDIENTE}.\\\"}\\n\\nREQ_ANT = \\\"Onde compro o Dipirona?\\\"\\nRESP_ANT = \\\"Olá, Você pode comprar na farmácia ...\\\"\\n...\\n{\\\"role\\\": \\\"system\\\", \\\"content\\\": F\\\"Anteriormente o usuário solicitou '{REQ_ANT}' e você respondeu '{RESP_ANT}'.\\\"}\\n\\n\\nRole User\\n\\nO role user é onde as mensagens do usuário serão inseridas. No exemplo acima foi a mensagem “Qual o sentido da vida?”.\\n\\nLimitações\\n\\nCada requisição é única e não tem memória. Ele não se lembra das requisições anteriores. Para que o modelo saiba o que foi dito anteriormente e ele possa continuar a conversa o histórico deve ser passado no role assistant.\\nTambém há um limite de informações que pode ser colocado nos roles. O conjunto dos roles é chamado contexto.  O limite do contexto é dado em tokens e atualmente é 4096 tokens, mas há modelos que tem 16k tokens.\\n\\nTokens podem ser uma letra ou uma palavra inteira, depende do vocabulário usado pelo modelo. Não conte as letras para verificar o limite de tokens, pois como dito, os tokens podem ser uma letra ou um pedaço de uma palavra ou uma palavra inteira.\\n\\nOs modelos tem controle de conversas tóxicas, ilegais, abusivas e temas sensíveis. Não peça para o modelo contar um conto erótico que dificilmente ele o fará. Também não é possível inventar histórias mentirosas de pessoas ou temas sensíveis. O modelo tem personalidade multicultural, feminista e woke.\\n\\nQuando acabar seus créditos só será possível continuar requisitando com um cartão de crédito onde será cobrado por tokens. Não é muito, você brincando sozinho consumirá menos de \\\\$ 1,00 ou em uso mais intensivo \\\\$ 5,00. Você pode configurar um limite no site para no caso de passar desse valor ele pare de responder as requisições.\\n\\nEmpresas\\n\\nAlgumas observações são importantes quando for usar o chatbot em empresas. Vamos imaginar um cenário onde o chatbot deverá ser usado para consultar um grande banco de dados de produtos e como se percebeu a limitação do contexto (os roles system, assistant e user) não tem como colocar todos os produtos e suas caracteristicas no contexto da requisição, isso é impraticável.\\n\\nPara que uma grande base de dados seja ‘ligada’ ao chatbot deverá ser usado um Langchain ou um Information Extration.\\nO Langchain usa duas camadas. A primeira etapa é a indexação da base de dados que você quer disponibilizar ao chatbot. Nesta etapa documentos, pdfs, textos, planilhas são indexadas por um modelo de embedded que gerará um grande vetor de números de ponto flutuante. Esse vetor representará uma sentença do texto e será armazenado em um banco de dados vetorial ( Vector Database ).\\n\\nQuando o usuário fizer uma requisição a requisição irá novamente para esse modelo embedded que gerará um vetor embedded representando a requisição e o banco de dados será consultado com o valor que mais se aproxima do vetor da requisição.\\nEncontrado a informação, a informação é retornada para o chatbot para ele gerar uma resposta com a informação obtida do banco de dados.\\n\\nExemplo:\\n\\nUm escritório de advocacia quer um chatbot ajudando seus advogados.\\n\\nPrimeira etapa ( pode ser feita somente uma vez ):\\n\\n  Reunir todos os documentos, leis, sentenças e diário oficiais que achar necessário.\\n  Extrair as sentenças ( por exemplo fazendo um .split(\\\".\\\") no texto ).\\n  Gerar um vetor embedded para cada sentença.\\n  Amazenar os vetores embedded em um banco de dados vetorial.\\n\\n\\nSegunda etapa (a cada requisição ):\\n\\n  O usuário digita uma requisição por exemplo “O Sr. Antonio de oliveira tem algum processo cível?”\\n  O modelo embedded gera um vetor com o texto da requisição.\\n  O banco de dados é consultado com esse vetor para achar a sentença mais próxima.\\n  Ele achou por exemplo “Antonio de oliveira x José no nascimento”\\n  A informação é adicionada ao role assistant para o chatbot gerar uma resposta.\\n\\n\\nO Information Extration é diferente este não usa um modelo embedded. Ele extrai a intenção do usuário e as palavras chaves que serão usados para montar diretamente uma requisição SQL ao banco de dados que então retornará a informação a ser inserida nos roles da requisição da resposta.\\n\\nLangchain e Information Extraction são áreas e ferramentas diferentes mas auxiliares do modelos de linguagem. A OpenAI e outras empresas disponibilizam esses serviços pagos.\\n\\n\\n  \\n    \\n      https://grok.x.ai/ &#8617;\\n    \\n    \\n      https://www.nytimes.com/2023/12/22/technology/apple-ai-news-publishers.html &#8617;\\n    \\n  \\n\\n\", \"O problema da mochila ou em inglês “knapsack” é um problema clássico de otimização combinatória.\\n\\nO problema consiste em colocar em uma mochila o maior valor possível de itens com determinada capacidade de peso. A cada item é associado um peso e um valor. O valor pode ser a utilidade ou o preço. Ao ir para o camping espera-se que a mochila seja carregada da maior utilidade de itens possível, enquanto na mochila de um motoqueiro o desejado é transportar o maior valor possível. Dessa forma, o objetivo da otimização é ter o maior valor dentro da mochila, ou seja, maximizar o valor sem ultrapassar o peso. Alguns itens podem ser muito valiosos porém pesados, enquantos alguns itens leves podem ter valor pequeno. Um algoritmo trivial de otimização irá na tentativa e erro colocar itens na mochila até alcançar o peso permitido e ir trocando os itens procurando sempre maximizar o valor dentro da mochila.\\n\\nEsse problema é generalizado para outras aplicações onde o peso pode ser qualquer variável de penalidade e o preço uma variável de maximização. Exemplos não faltam:\\n\\n\\n  Embarcar a maior quantidade de itens possíveis em um caminhão com custo de carga alto\\n  Alocar máquinas de produção visando o menor tempo possível e maior valor produzido\\n  Corte de chapas de madeira, metal, tecidos\\n  Empacotamento\\n  Investimento de Capital\\n\\n\\nCVXPY\\n\\nCVXPY é um framework python para diversos otimizadores com uma linguagem própria e suporte a Numpy.\\n\\nPara instalar o CVXPY use o pip:\\n\\npip install cvxpy\\n\\nUm dos otimizadores disponíveis é o CBC  um otimizador de programação linear inteiro open-source. O CBC pode ser usado em linha de comando sem ser necessário o CVXPY, mas neste caso será necessário aprender sua linguagem de entrada. \\n\\nTambém será necessário instalar o CVXOPT:\\n\\npip install cvxopt\\n\\nEsse problema aparece em diversas áreas como a logística, computação e investimentos.\\n\\nimport cvxpy as cp\\nimport numpy as np\\n\\nDados de entrada do problema\\n\\nValores = np.array([10,13,1,100,45,13,156,76,4,59,97,99])\\nPesos = np.array([50,55,10,5,1,98,34,3,9,3,7,19])\\nCapacidade_Mochila = 100\\n\\nVariaveis de decisão\\n\\nCada Item Xi terá valor 1 se estiver na mochila ou 0 se estiver fora cp.Variable cria uma variável no CVX ( não confunda cp com np ) do tipo boolean do tamanho da quantidade de itens\\n\\nXi = cp.Variable((Valores.size), boolean = True)\\n\\nConstraints do problema\\n\\nA soma total dos pesos dos itens escolhidos por Xi devem ser igual ou menor que a capacidade da mochila                \\n\\n1\\nconstraints = [ Xi @ Pesos &lt;= Capacidade_Mochila ]\\n\\n\\nTambem pode ser escrito com o mesmo resultado como:\\n\\n1\\nconstraints = [ cp.sum( cp.multiply (Xi, Pesos ) ) &lt;= Capacidade_Mochila ]\\n\\n\\nO Objetivo do problema é maximizar os valores na mochila\\n\\n1\\nobjective = cp.Maximize( Xi @ Valores )\\n\\n\\nfinalmente chamamos o solver com verbose para acompanhar \\no progresso e a execução máxima de 1hr\\n\\n1\\n2\\n3\\n4\\n5\\nprob = cp.Problem(objective, constraints)\\nprob.solve(solver=cp.CBC,verbose=True, maximumSeconds = 1 * 60 * 60) \\nprint(\\\"Status          : \\\", prob.status)\\nprint(\\\"Valor encontrado: \\\", prob.value)\\nprint(\\\"Valor de Xi     : \\\", Xi.value)\\n\\n\\nO problema da mochila tem muitas aplicações práticas principalmente na logística. Imagine uma transportadora que precisa distribuir seus pacotes utilizando vans e caminhões. Cada pacote tem peso, tamanho, localidade, tempo de espera e valor. Um programa de otimização pode ajudar no preenchimento dos caminhões e em aplicações profissionais traçar a rota de distribuição de cada veículo.\\n\\nProgramas de otimização não acham o melhor valor, porque em geral é uma tarefa impossível (o mais correto seria improvável) de se achar, devido a explosão combinatória. Por isso são chamados ‘otimizadores’ pois eles buscam a melhor solução possível, ou seja, uma solução ótima.\\n\", \"Todas as coisas são números.\\nTransformando idéias e sensações em números\\n\\nO mundo não é determinístico para nós. Não temos as fórmulas, as regras, as leis, ou a onisciência de saber quando, quanto e se a coisas acontecerão no futuro, entretanto ainda conseguimos perceber que alguns eventos são mais prováveis de ocorrer que outros; sabemos naturalmente que em céu nublado é mais provável de chover do que em céu limpo, que ganhar na loteria é mais improvável que ganhar em um jogo de cartas.\\n\\nPorém confiar somente nas vagas sensações de que algo acontecerá poderia trazer prejuízos enormes. Os egipicios buscavam prever as cheias do Rio Nilo para não passarem fome, investidores precisam medir a viabilidade de um negócio novo, transportadoras e agências de seguros não sobrevivem sem mensurar as chances de um acidente ocorrer. Transformar idéias e conceitos em números foi se tornando algo útil e essa quantificação das idéias chegou nas ‘possibilidades’ para nos ajudar a prever o futuro desconhecido.\\n\\nE assim na busca pelo dinheiro fácil nos jogos que a matemática de tentar prever o futuro avançou e tornou-se o que conhecemos hoje como probabilidade. Dessa forma era necessário um número fixo e confiável, um valor que mostrasse o quanto uma aposta era mais provável de ganhar que perder, já não bastava apostar um bom dinheiro apenas na intuição “essa jogada tem altas chances de ganhar”, era necessário mais precisão.\\n\\nOs jogos em geral são sistemas fáceis de se lidar matematicamente, são poucas as variáveis e seus valores estão acessíveis. Um dado lançado tem 6 valores possíveis, uma carta tirada de um baralho completo virá de uma das 52 cartas. Uma moeda só tem dois lados e quando se lança somente dois resultados possíveis poderão ocorrer: a cara ou a coroa. Chamamos esses resultados de eventos de um experimento. Jogar um dado e anotar a face de cima é um experimento, o valor da face é o evento.\\n\\nImagine que sua chance de ficar rico seja o lançar de um dado de 6 lados. Se você acertar o número que aparecer na face de cima após um lançamento você ficará rico. Se o dado tem 6 lados então temos 5 possibilidades de perder e somente 1 possibilidade de ganhar. E se for um dado de 20 lados? Agora são 19 para 1 a possibilidade de perder. E se lhe fosse permitido escolher 5 números no dado de 6? Então neste caso você teria 5 chances de ganhar e 1 de perder. Seria muito azar não ficar rico.\\n\\nNão confunda as chances, possibilidades e tentativas. Há apenas uma tentativa em nosso jogo. Perdeu no primeiro lançamento, acabou o sonho. Se for uma moeda você já abre o champanhe antes mesmo de lançar pois as chances são grandes. Se for na Mega Sena você não dá bola, pois sabe que vai perder.\\n\\n\\nProbabilidade Clássica\\n\\nNa probabilidade clássica considera-se que os eventos tem a mesma probabilidade de ocorrerem, ou seja, são equiprováveis. Se você pensar no lançamento do dado a chance de cair qualquer número é igual para todos. É o mesmo caso do lançamento da moeda no cara e coroa ou na retirada do baralho.\\n\\nO conjunto de valores possíveis que um evento pode ser se chama Espaço Amostral e é abreviado pela letra ‘S’. A tabela a seguir ilustra o espaço amostral dos exemplos mais comuns:\\n\\n\\n  \\n    \\n       \\n      Valores possíveis\\n      Espaço amostral\\n      Número\\n    \\n  \\n  \\n    \\n      Elementos\\n      1,2,3,4,5,6\\n      $S={1,2,3,4,5,6}$\\n      6\\n    \\n    \\n      Moeda\\n      Cara,Coroa\\n      $S={Cara,Coroa}$\\n      2\\n    \\n    \\n      Baralho 52 cartas\\n      Às de espada,Reis de ouro, 2 de copas, …\\n      $S={\\\\text{Às de espada}, …\\\\ \\\\text{Reis de Ouro}}$\\n      52\\n    \\n  \\n\\n\\nNa probabilidade clássica como cada elemento do conjunto do espaço tem a mesma probabilidade de ocorrer dividimos 1 (ou 100%) pelo número de eventos possíveis, ou seja, a quantidade de elementos do Espaço amostral para se obter a probabilidade da ocorrência de cada elemento.\\nQuanto maior o Espaço Amostral menores as chances de um evento ocorrer, como se vê na tabela a seguir.\\n\\n\\n  \\n    \\n       \\n      Valores possíveis\\n      Espaço amostral\\n      Probabilidade\\n    \\n  \\n  \\n    \\n      Dado\\n      $S={1,2,3,4,5,6}$\\n      6\\n      $P=\\\\frac{1}{6}=0.16…$\\n    \\n    \\n      Moeda\\n      $S={Cara,Coroa}$\\n      2\\n      $P=\\\\frac{1}{2}=0.50$\\n    \\n    \\n      Baralho\\n      $S={\\\\text{Às de espada}, …\\\\ \\\\text{Reis de Ouro}}$\\n      52\\n      $P=\\\\frac{1}{52}=0.019$\\n    \\n  \\n\\n\\nComo se pode ver a probabilidade clássica atribui um número igual para todos os eventos.\\n\\nO problema da probabilidade clássica é assumir que todos os elementos do espaço amostral tem a mesma probabilidade e conhecer o número de elementos do espaço amostral, o que nem sempre ocorre na vida real.\\n\\nProbabilidade frequentista\\n\\nA probabilidade se mostrou útil para outras áreas além dos jogos, porém em outras áreas não é possível calcular a probabilidade de um evento ocorrer com uma simples divisão. Em uma indústria de lampadas como saber qual a probabilidade de uma lampada queimar depois de 1000 horas de uso? O senso comum costuma dizer que “Queima ou não queima, então são 50% de queimar.”. Não, não é assim que funciona.\\n\\nUma outra forma de atribuir probabilidades a eventos é observando diversos experimentos reais e calcular os eventos ocorridos pelo número de observações.\\n\\nExemplos:\\n\\nUma indústria testou 100 lampadas ligadas por 1000 horas e depois de 1000 horas 7 delas queimaram.  Qual a probabilidade que uma lampada dessa indústria queime com 1000 horas de uso?\\n\\nEfetuando os cálculos:\\n\\n$P=\\\\frac{7}{100}=0.07=7\\\\%$\\n\\nHá 7% de chance de uma lampada dessa indústria queimar com 1000 horas de uso.\\n\\nUm cassino estava sob suspeita de fraude e desconfiou-se que seus dados não são confiáveis. Para dirimir as dúvidas foram lançados 50 vezes o dado e obtido os seguintes valores:\\n\\nValor123456\\nExibições988889\\nProbabilidade988889\\n\\nDetermine se o dado é confiável.\\n\\n\\nEfetuando os cálculos:\\n\\n\\n  \\n    \\n      Exibições\\n      9\\n      8\\n      8\\n      8\\n      8\\n      9\\n    \\n    \\n      Probabilidade\\n      $\\\\frac{9}{50}=0.18$\\n      $\\\\frac{8}{50}=0.16$\\n      $\\\\frac{8}{50}=0.16$\\n      $\\\\frac{8}{50}=0.16$\\n      $\\\\frac{8}{50}=0.16$\\n      $\\\\frac{9}{50}=0.18$\\n    \\n  \\n\\n\\nComo a probabilidade de um dado confiável (dado pela probabilidade clássica) é $\\\\frac{1}{6}=0.16$ podemos concluir que o dado é confiável.1\\n\\nA probabilidade frequentista tem a desvantagem de ser necessário realizar os experimentos para a obtenção dos dados o que em alguns casos é impraticável, no exemplo da indústria de lâmpadas seria preciso esperar um bom tempo e destruir as lampadas para a obtenção dos dados. Outro problema é saber quantos experimentos realizar. A teoria diz que o correto é a realização de infinitos testes, algo impossível de se realizar.\\n\\nComparada a anterior, a visão frequentista é mais eficiente em determinar as probabilidades de eventos diferentes do espaço amostral do que simplesmente assumir que os eventos são equiprováveis.\\n\\nProbabilidade subjetiva\\n\\nEnquanto a probabilidade clássica tem total controle sobre o espaço amostral e seus valores e na probabilidade frequentista experimentos são feitos para se obter as probabilidades dos eventos, na probabilidade subjetiva as chances dos eventos ocorrerem é uma opinião pessoal.\\n\\nDe fato é preferível valores precisos da visão frequentista mas em alguns casos não há histórico das observações ou a possibilidade de realizar os experimentos. Suponha um escritório de advocacia que recebeu um processo de um cliente e este pergunta as chances de se ter vitória? Como calcular isso matematicamente? Em geral, o advogado mais experiente faz as melhores previsões.\\n\\nNos cargos de gerência e situações de tomada de decisões um dos atributos mais importantes é perceber corretamente as probabilidades dos eventos. 2\\n\\n\\nCuriosidade:\\n\\nO big data é uma grande quantidade de dados de alguma área registrados em computador. Ao dirigir usando o GPS este registra o trânsito, tempo de viagem, percurso e ocorrências de todas as pessoas que usam ele; essa grande quantidade de dados com o uso de computadores ajuda o GPS a calcular a probabilidade e o tempo de viagem em um dado dia e horário.\\n\\nOs escritórios de advocacia e os tribunais tem registro de milhares de processos que estão sendo fontes de big data para que programas calculem, usando métodos frequentistas e de processamento de linguagem, as chances dos litígios.\\n\\n\\n\\n  \\n    \\n      Para um leitor de nível superior a conclusão não segue o rito normal da inferência estatística. Porém o artigo é direcionado ao nível médio e o leitor pode concordar que intuitivamente é uma conclusão válida. &#8617;\\n    \\n    \\n      Read, D. (2005). Judgment and Choice. In K. Kempf-Leonard (Ed.), Encyclopedia of Social Measurement (pp. 401–407). Elsevier. &#8617;\\n    \\n  \\n\\n\", \"Uma Introdução à Inferência Estatística\\n\\nInferência estatística é uma parte crucial da estatística que nos permite tomar decisões a partir dos dados. É uma ferramenta essencial para analisar uma grande variedade de dados e encontrar respostas a questões específicas.\\n\\nDefinição de Inferência Estatística\\n\\nA Inferência estatística é o processo de usar a análise de dados para deduzir propriedades de uma população ou distribuição de probabilidade. Envolve análise estatística de uma amostra dos dados, gerada pela população. A inferência estatística inclui estimativa de parâmetros e teste de hipóteses.\\n\\nTipos de Inferência Estatística\\n\\nExistem dois tipos principais de inferência estatística: estimação de parâmetros e teste de hipóteses.\\n\\n\\n  Estimação de parâmetros: Isso envolve chegar a uma estimativa da medida de um população a partir de uma amostra. A estimativa pode ser expressa através de um único valor (estimativa pontual) ou um intervalo de valores (estimação intervalar ou intervalo de confiança).\\n\\n\\nPor exemplo, suponha que você queira saber a altura média dos alunos em uma universidade. Para isso, em vez de medir a altura de todos os alunos, você mede a altura de 100 alunos selecionados aleatoriamente (a amostra) e, com base nesses dados, você estima a altura média de todos os alunos (a população).\\n\\n\\n  Teste de hipóteses: Com base na amostra, uma afirmação é feita sobre a população. Essa afirmação (chamada de hipótese) é testada para aceitação ou rejeição. A hipótese de teste é apenas uma suposição sobre a população de onde a amostra foi retirada.\\n\\n\\nPor exemplo, se uma empresa de laticínios afirma que a média do teor de gordura de seu leite é de 3,5%, um consumidor preocupado com a saúde pode pegar uma amostra do leite e testar a hipótese de que o teor de gordura média é realmente esse valor.\\n\\nPor que a Inferência Estatística é Importante?\\n\\nA inferência estatística desempenha um papel vital na análise de dados. Ela fornece métodos para quantificar a incerteza nas conclusões e usar os dados para tomar decisões informadas. É uma ferramenta fundamental para prever tendências futuras, testar teorias e hipóteses, e tomar decisões estratégicas em negócios e pesquisa.\\n\\nExemplo\\n\\nDigamos que uma empresa deseja entender quanto tempo seus funcionários gastam em uma tarefa específica para melhorar a eficácia, e para isso colheu uma amostra de 25 funcionários.\\n\\nDado os tempos em minutos:\\n\\n\\n  \\n    \\n      Tempo em minutos\\n      20, 24, 22, 25, 23, 23, 21, 21, 22, 24, 22, 21, 24, 26, 25, 23, 22, 24, 23, 26, 27, 22, 24, 23, 22\\n    \\n  \\n\\n\\nPrimeiro, calculamos a média e o desvio padrão dessa amostra:\\n\\n  Média $\\\\bar{X} = {\\\\text{soma de todos os tempos} \\\\over \\\\text{número de amostras}}$\\n  Desvio padrão $s = {\\\\sqrt {\\\\sum{(X_i - \\\\bar{X})^2} \\\\over {\\\\text{número de amostras} - 1}}}$\\n\\n\\nCalculando com os valores dados obtemos as seguintes informações:\\n\\n\\n  \\n    \\n      Média $\\\\bar{X}$\\n      23.16\\n    \\n    \\n      Desvio padrão $s$\\n      1.71\\n    \\n  \\n\\n\\nEm seguida, podemos usar essas informações para criar um intervalo de confiança de 95% para o tempo médio que todos os funcionários gastam na tarefa. Supomos que as medidas seguem uma distribuição normal.\\n\\n\\n  $Z_{score}$ para a confiança de $95\\\\% = 1.96$ (Consultando a tabela Z-Score)\\n  Erro padrão $SE = {s \\\\over \\\\sqrt{\\\\text{número de amostras}}}$\\n  Intervalo de confiança = $(\\\\bar{X}) ± Z_{score} \\\\times SE$\\n\\n\\nUma vez que calculamos o intervalo de confiança, podemos dizer que estamos 95% confiantes de que o tempo médio real que todos os funcionários gastam na tarefa está dentro do intervalo de confiança.\\n\\nObserve que as fórmulas acima são simplificações e assumem uma distribuição normal e uma amostra grande o suficiente. Os cálculos exatos podem ser mais complexos dependendo dos aspectos específicos dos dados e do que planejamos inferir.\\n\\nA inferência estatística é uma parte indispensável da estatística e é usada em várias disciplinas que envolvem tomada de decisões baseadas em dados. Também é essencial para traduzir os dados recolhidos em informações compreensíveis e gerenciáveis, e para transformar essas informações em conhecimento para a tomada de decisões.\\n\\nIndependentemente de se tratar de negócios, medicina, engenharia ou ciências sociais, a inferência estatística continuará a ser um componente fundamental na coleta, análise e interpretação de dados, permitindo que as pessoas tomem decisões bem fundamentadas.\\n\", \"Usando Python para ODEs de circuitos elétricos: RL\\n\\nNeste artigo explorarei a solução de circuitos elétricos utilizando Python. A solução analítica será encontrada utilizando SymPy enquanto a solução numérica será obtida com SciPy e Numpy.\\n\\nTratarei apenas do circuito RL da figura abaixo pois encontramos neste tipo de circuito apenas uma derivada. Estudaremos os casos para fontes DC e AC.\\n\\n\\n\\nPara uma fonte DC de 5V o circuito é modelado pela seguinte equação diferencial\\n\\n\\\\[5 = 4700 i{\\\\left(t \\\\right)} + 0.0001 \\\\frac{d}{d t} i{\\\\left(t \\\\right)}\\\\]\\n\\nEsse é um exemplo de Equação Diferencial Ordinária ou abreviadamente ODE cujas soluções podem ser analíticas ou numéricas. A solução analítica é a solução como conhecemos na escola ou a simples solução algébrica através de manipulação dos símbolos onde obteremos no final uma família de funções matemáticas em que podemos calcular o comportamento do circuito. Obter a solução analítica de uma ODE requer conhecimentos de cálculo 1 e 2 lecionados no nível superior, mas utilizando algum CAS (Computer Álgebra System) a solução pode ser encontrada rapidamente.\\n\\nA solução numérica não manipula símbolos, utiliza-se de métodos interativos com tentativa e erros com um certo discernimento de para onde ir em busca da solução. Pode ser feito na mão ou mais adequadamente no computador.\\n\\nRequerimentos\\n\\nComo as soluções serão procuradas com Python precisaremos dos pacotes SymPy, Numpy, Matplotlib e SciPy que podem ser facilmente instalados utilizando o pip:\\n\\npip install sympy numpy matplotlib scipy\\n\\n\\n\\nConheça os pacotes usados\\n\\n    Numpy - Pacote de manipulação de vetores, matrizes e operações matemáticas em geral. Utiliza internamente o OpenBLAS. Praticamente utilizado por todos os outros pacotes inclusive todos os abaixos.\\n    Sympy - Pacote de computação matemática simbólica\\n    Matplotlib - Exibe gráficos.\\n    Scipy - Um grande pacote com diversas utilidades em vários ramos da ciência.\\n\\n\\n\\nEm todos os arquivos os imports necessários e a cabeça do arquivo será este:\\n\\n\\nfrom sympy import *\\nimport numpy as np\\nfrom scipy.integrate import solve_ivp\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n\\nVersão analítica com SymPy\\n\\nA solução analítica pode ser encontrada utilizando o módulo Sympy. É um módulo promissor, mas ainda está em um nível intermediário de maturidade. Não se compara em poder ao Maxima e ao Mathematica.\\n\\nA solução analítica de uma ODE utilizando o Sympy é encontrada com o comando dsolve.\\n\\n\\nS = 5\\nR = 4700\\nL = 100e-6\\n\\nt = Symbol('t')\\nv = Function('v')(t)\\ni = Function('i')(t)\\n    \\neq = Eq(S,R*i+L*i.diff(t))\\neq\\n\\nO Sympy exibe em formato tex a equação acima como:\\n\\n\\\\[5 = 4700 i{\\\\left(t \\\\right)} + 0.0001 \\\\frac{d}{d t} i{\\\\left(t \\\\right)}\\\\]\\n\\nE como dito a solução analítica é encontrada pelo comando dsolve:\\n\\ndsolve(eq)\\n\\n\\ncuja saída será:\\n\\n\\\\[\\\\displaystyle i{\\\\left(t \\\\right)} = C_{1} e^{- 47000000.0 t} + \\\\frac{1}{940}\\\\]\\n\\nQue é a função de $i$ em $t$ como desejamos. $C_1$ é uma constante que como toda equação diferencial e sistemas físicos que armazenam energia essa constante define a condição inicial da função, oras, temos um indutor no circuito que INICIALMENTE pode ter um campo magnético de valor 0 ou qualquer outro valor que iria interferir no início do circuito. Cabe a nós dizer qual é esse campo. Mas em geral começamos com condições iniciais com corrente zero, ou seja, $i(t=0) = 0$.\\n\\nVersão numérica com SciPy e Numpy\\n\\nSim é muito Py. Pois este é poder do Python: Seus módulos! E as pessoas gostam de homenagear o Python colocando o Py no nome.\\n\\nA solução numérica será encontrada utilizando o comando solve_ivp porém o correto agora é dizermos “as soluções” pois o que obteremos é uma tabela (List) com os valores numéricos de $i$ em $t$ que utilizaremos para visualizar o gráfico do comportamento da corrente no circuito.\\n\\nO solve_ivp precisa que a equação esteja no formato\\n\\n\\\\[{dy(t) \\\\over dt}= ...\\\\]\\n\\nque no nosso caso então seria\\n\\n\\\\[{di(t) \\\\over dt}= ...\\\\]\\n\\ne que esta equação esteja em forma de função Python, dessa forma definimos então a função circuito:\\n\\n\\ndef circuito(t,i):\\n    return (5 -4700*i)/0.0001\\n    \\ntempo_maximo = 0.000001\\nt = np.linspace(0,tempo_maximo,1000)\\nsol = solve_ivp(circuito,[0,tempo_maximo],[0],method='RK45',t_eval=t)\\nplt.plot(sol.t,sol.y[0])\\n\\n\\nO primeiro parâmetro de solve_ivp é nossa função, o segundo o intervalo de tempo que a ODE será integrada e o [0] é o valor inicial e como foi dito antes a corrente no circuito inicialmente é zero, ou seja [0].\\n\\nTempo_maximo é crítico. Se um valor maior que o transitório do circuito for colocado não visualizaremos nada de útil, somente o estado estacionário do circuito. Se for muito pequeno veremos apenas o início do estado transitório que provavelmente será uma reta. Com os valores do programa o seguinte gráfico aparecerá:\\n\\n\\n\\nComo a fonte é contínua o gráfico se apresenta como o esperado. A corrente começa do zero e vai subindo gradativamente até que o indutor seja somente um curto-circuito e a corrente circulante seja a limitada pelo resistor.\\n\\nFonte de tensão senoidal\\n\\nUm circuito de corrente contínua não nos oferece muito, vamos então usar uma fonte de tensão de 1Khz com Vp de 5V.\\n\\nSolução analítica\\n\\n\\nS = 5*sin(t*(2*pi/(1/1000)))\\nR = 4700\\nL = 100e-6\\n\\nt = Symbol('t')\\nv = Function('v')(t)\\ni = Function('i')(t)\\n    \\neq = Eq(S,R*i+L*i.diff(t))\\neq\\ndsolve(eq)\\n\\n\\nA única diferença é a mudança do S com a senoide provida pelo Seno do SymPy sin. As saídas serão então:\\n\\n\\\\[\\\\displaystyle 5 \\\\sin{\\\\left(2000 \\\\pi t \\\\right)} = 4700 i{\\\\left(t \\\\right)} + 0.0001 \\\\frac{d}{d t} i{\\\\left(t \\\\right)}\\\\]\\n\\nE a solução\\n\\n\\\\[\\\\displaystyle i{\\\\left(t \\\\right)} = C_{1} e^{- 47000000.0 t} + 0.00106382976822168 \\\\sin{\\\\left(2000 \\\\pi t \\\\right)} - 1.42217863170866 \\\\cdot 10^{-7} \\\\cos{\\\\left(2000 \\\\pi t \\\\right)}\\\\]\\n\\nBasicamente uma corrente não defasada com 1.06mA. Não entendi o outro cos de valor irrisório. Provavelmente o transitório inicial não seja uma composição pura do exponente e o seno, mas tenha algum componente que deforme a onda (Fourier).\\n\\nSolução numérica\\n\\nJá na solução numérica faremos maiores alterações.\\n\\n\\ndef circuito(t,i):\\n    return (5*np.sin(t*(2*np.pi/0.0001)) -4700*i)/0.0001\\n    \\ntempo_maximo = 0.001\\nt = np.linspace(0,tempo_maximo,10000)\\nsol = solve_ivp(circuito,[0,tempo_maximo],[0],method='RK45',t_eval=t)\\nprint(max(sol.y[0]),min(sol.y[0]))\\n\\nplt.subplots(figsize=(15, 5))\\nplt.subplot(1, 2, 1)\\nplt.plot(sol.t,sol.y[0])\\nplt.subplot(1, 2, 2)\\nplt.plot(t,5*np.sin(t*(2*np.pi/0.0001)))\\n\\n\\nAs alterações, além da fonte, foi o tempo que foi diminuído e exibimos o i máximo e mínimo como mostrado abaixo:\\n\\n\\n  \\n    \\n      i máximo\\n      0.0010661978534370956\\n    \\n    \\n      i minimo\\n      -0.0010652848773317844\\n    \\n  \\n\\n\\nNos gráficos abaixo o primeiro é a corrente do circuito encontrada pela solução numérica e o outro a tensão da fonte.\\n\\n\\n\\nOs valores mínimos e máximos mostram uma corrente de 1.06mA que confere com a solução analítica obtida anteriormente.\\n\\nFalstad\\n\\nVamos conferir com o simulador de circuitos do Falstad? Que aliás usa de muita solução numérica para simular o circuito.\\n\\n\\n\\nComo se pode ver no osciloscópio Fonte CA o Max=1.064mA novamente confere com os cálculos.\\n\\nLink para o ipynb (Jupyter Notebook) deste post.\\n\\nEspero que tenham gostado. Em breve novos posts!\\n\\n\", \"O que são Filtros FIR?\\n\\nOs Filtros de Resposta ao Impulso Finita, ou FIR (do inglês Finite Impulse Response), são uma classe de filtros digitais amplamente utilizados em processamento de sinais. Ao contrário dos Filtros de Resposta ao Impulso Infinita (IIR), os FIR têm uma resposta ao impulso de duração finita, o que os torna mais simples de entender e implementar.\\n\\nOs filtros FIR são frequentemente utilizados em uma variedade de aplicações, como processamento de áudio, comunicação digital e processamento de imagens. Sua característica finita permite uma análise mais fácil de seu comportamento e uma implementação mais robusta em muitas situações.\\n\\nEstrutura Básica dos Filtros FIR\\n\\nA estrutura básica de um filtro FIR consiste em uma soma ponderada de amostras de entrada, cada uma multiplicada por um coeficiente específico. Matematicamente, a saída $y[n]$ de um filtro FIR pode ser expressa como a soma ponderada das amostras de entrada $x[n]$ e seus coeficientes associados $h[k]$:\\n\\n\\\\[ y[n] = \\\\sum_{k=0}^{N-1} h[k] \\\\cdot x[n-k] \\\\]\\n\\nOnde:\\n\\n  $N$ é a ordem do filtro (o número total de coeficientes),\\n  $x[n]$ é a amostra de entrada no instante $n$,\\n  $h[k]$ são os coeficientes do filtro.\\n\\n\\nExemplo de Cálculo de Coeficientes FIR\\n\\nPara entender melhor como os coeficientes de um filtro FIR são calculados, consideremos um exemplo prático. Vamos projetar um filtro passa-baixa com uma frequência de corte normalizada de 0,2 em um sistema com frequência de amostragem normalizada de 1.\\n\\n\\n  \\n    Determine a ordem do filtro (N):\\n  A ordem do filtro depende da complexidade desejada. Para este exemplo, escolheremos uma ordem $N = 15$.\\n  \\n  \\n    Calcule a frequência de corte digital (wc):\\n  A frequência de corte digital é obtida multiplicando a frequência de corte desejada pela frequência de amostragem. Portanto, $wc = 0,2 \\\\times 1 = 0,2$.\\n  \\n  \\n    Calcule os coeficientes (h[k]):\\n  Utilizaremos a fórmula para um filtro passa-baixa ideal:\\n\\n    $ h[k] = \\\\frac{\\\\sin(\\\\pi \\\\cdot k \\\\cdot wc)}{\\\\pi \\\\cdot k} $\\n\\n    Para $k = 0$, temos que $h[0] = 2 \\\\cdot wc$. Para $k$ diferente de zero, aplicamos a fórmula.\\n\\n    Vamos calcular os coeficientes para $k = 1$ até $N-1 = 14$.\\n\\n    $ h[k] = \\\\frac{\\\\sin(\\\\pi \\\\cdot k \\\\cdot 0,2)}{\\\\pi \\\\cdot k} $\\n\\n    Portanto, os coeficientes seriam:\\n  \\\\[ h[0] = 0,4 \\\\]\\n  \\\\[ h[k] = \\\\frac{\\\\sin(\\\\pi \\\\cdot k \\\\cdot 0,2)}{\\\\pi \\\\cdot k} \\\\quad \\\\text{para } k = 1 \\\\text{ até } 14 \\\\]\\n  \\n  \\n    Normalização dos Coeficientes:\\n  É comum normalizar os coeficientes de modo que a soma dos quadrados seja igual a 1.\\n\\n\\\\[h[k]_{\\\\text{normalizado}} = \\\\frac{h[k]}{\\\\sqrt{\\\\sum_{k=0}^{N-1} h[k]^2}}\\\\]\\n\\n    Isso garante que a amplitude do sinal não seja afetada durante o processo de filtragem.\\n  \\n\\n\\nOs filtros FIR são uma ferramenta poderosa no processamento de sinais digitais. Com uma compreensão básica de sua estrutura e uma abordagem prática para calcular os coeficientes, é possível projetar filtros personalizados para atender às necessidades específicas de uma aplicação. Experimentar com diferentes ordens e frequências de corte permite ajustar o desempenho do filtro de acordo com os requisitos do sistema.\\n\", \"A hipótese desempenha um papel fundamental na ciência, sendo uma proposição ainda sem valor conhecido, aguardando evidências adicionais para determinar sua veracidade ou falsidade. O processo de formular hipóteses muitas vezes envolve o raciocínio indutivo, onde a observação de casos particulares leva à proposição de uma afirmação geral. No entanto, a validade dessas hipóteses depende da verificação cuidadosa por meio de testes e evidências.\\n\\nEnquanto o senso comum muitas vezes aceita hipóteses sem questionamento, o método científico exige rigor. As hipóteses devem ser testadas e verificadas de maneira exaustiva. A origem duvidosa de uma hipótese é desconsiderada no método científico, algo que o senso comum muitas vezes negligencia.\\n\\nO rigor do método científico inclui não apenas a verificação da hipótese, mas também a documentação detalhada do processo de teste. Isso é essencial para garantir a transparência e a replicabilidade dos resultados. Um exemplo notável foi a hipótese sobre os efeitos prejudiciais do chumbo na gasolina1, onde a comunidade científica enfrentou resistência de grandes empresas, destacando a importância de uma abordagem meticulosa e documentada para validar as hipóteses.\\n\\nA questão da validação de hipóteses levanta o Problema da Demarcação, questionando como determinar o que é considerado ciência. Karl Popper propôs que uma característica crucial da ciência é a falsificabilidade - a capacidade de uma hipótese ser refutada. Se uma hipótese não pode ser falseada de alguma forma, ela não pode ser considerada científica. As ciências sociais são frequentemente criticadas por essa razão, pois algumas alegam que falta um método claro para rejeitar suas bases e hipóteses.\\n\\nEm resumo, a ciência exige rigor na formulação e verificação de hipóteses, com ênfase na falsificabilidade como critério distintivo. O método científico busca garantir que as afirmações sejam testadas de maneira objetiva e transparente, contribuindo para o avanço do conhecimento científico.\\n\\n\\n  \\n    \\n      Barrett, G. (2022, April 22). The man who accidentally killed the most people in history —. Veritasium. Verisatium &#8617;\\n    \\n  \\n\\n\", \"1 Tempos atrás (2013), com humor, um “surto de honestidade” entre os cientistas percorreu o twitter com a hashtag #overlyhonestmethods. Naquela semana os cientistas fugiram da impessoalidade e frieza dos artigos científicos e foram honestos com o público expondo como realmente suas descobertas aconteceram.\\n\\nTemos a sensação ao ler esses artigos que toda descoberta seja perfeita, siga um rito rigoroso e metódico e dessa forma continuamos confiantes na ciência. Mas nem sempre é assim, destaco a descoberta acidental da Penicilina que revolucionou a medicina no tratamento das infecções: De um acidente de laboratório descobriu-se os antibióticos, então, depois do êxtase da descoberta restava explicar o porque, o como e para que; cansativo não?\\n\\nPorque não se lembrar do técnico de manutenção de uma máquina que deu-lhe um chute voltando a funcionar e dado por seu turno acabado tão cedo ficou feliz, porém quando seu chefe lhe pede um relatório dos motivos, sua alegria se vai pois terá que realmente desmontar a máquina e pesquisar o componente defeituoso.\\n\\nO conhecimento de um fenômeno, ou descoberta pode ser acidental, criativo ou porque não vir dos Céusm porém o Conhecimento Científico exige que se explique o porquê, ou pelo menos o como para que outros possam reproduzir e testar a descoberta. Da mesma forma a investigação de um crime exige a minuciosa enumeração das causalidades que não devem deixar dúvidas de sua virtual reprodução.\\n\\nEntretanto o Conhecimento Filosófico não serve a ciência, aliás não serve a ninguém, ele somente duvida, não crê e indaga as verdades.\\n\\n- Uau! A Penicilina é uma grande descoberta! Muitas vidas foram salvas!\\n\\nMas ela cura? O que é a cura? Seus efeitos colaterais não seriam uma nova doença? Eu realmente estou saudável? Eu existo? Você existe?\\n\\nCertamente o técnico não pode se valer desses questionamentos com seu chefe sobre perigo de loucura e rua, mas terá que escrever um relatório no Word dos motivos do defeito e criar um procedimento reproduzível de como se precaver desse fenômeno (o defeito) indesejado.\\n\\nQuem quiser saber mais sobre a crise de honestidade dos cientistas\\n\\n\\n  \\n    \\n      Photo by British Library on Unsplash Photo by British Library on Unsplash &#8617;\\n    \\n  \\n\\n\"]",
    "excerpt" : "[\"Órgãos Antigos dos Anos 70: Geradores e Divisores de Oitavas\\n\\n\", \"Se você tem um leitor OBD2 em inglês essa lista pode te ajudar a desvendar as siglas na leitura.\\n\\n\", \"Os problemas primal e dual estão intrinsecamente ligados na teoria da dualidade em programação linear. Eles representam duas formulações diferentes de um mesmo problema de otimização linear, sendo inter-relacionados de maneira específica.\\n\\n\", \"Em 2023 Taylor Swift foi eleita a personalidade do ano, mas há quem diga que 2023 foi mesmo o ano da Inteligência Artificial. E de fato há uma corrida das empresas que ficaram para trás do lançamento do modelo  text-davinci da OpenAI que era um modelo rudimentar comparado aos gpt-turbo-3.5 e o mais moderno gpt-4 usados hoje, mas fez barulho. O Google lançou o Bard mas não impressionou. A Baidu lançou seu modelo porém restrito ainda a sua área, enquanto a Amazon e a Apple correm atrás do prejuízo. Elon Musk lançou o Grok1, mas até o momento tinha fila de espera para usar. A Apple tenta negociar um acordo milionário para poder usar os artigos da imprensa americana para seu modelo de linguagem2. A Microsoft foi astuta e financiou a OpenAI quando percebeu o potencial do lançamento do GPT-2 e hoje oferece esses modernos modelos como serviço pago ou gratuito.\\n\\n\\n  \\n    \\n      https://grok.x.ai/ &#8617;\\n    \\n    \\n      https://www.nytimes.com/2023/12/22/technology/apple-ai-news-publishers.html &#8617;\\n    \\n  \\n\\n\", \"O problema da mochila ou em inglês “knapsack” é um problema clássico de otimização combinatória.\\n\\n\", \"Todas as coisas são números.\\nTransformando idéias e sensações em números\\n\\n\", \"Uma Introdução à Inferência Estatística\\n\\n\", \"Usando Python para ODEs de circuitos elétricos: RL\\n\\n\", \"O que são Filtros FIR?\\n\\n\", \"A hipótese desempenha um papel fundamental na ciência, sendo uma proposição ainda sem valor conhecido, aguardando evidências adicionais para determinar sua veracidade ou falsidade. O processo de formular hipóteses muitas vezes envolve o raciocínio indutivo, onde a observação de casos particulares leva à proposição de uma afirmação geral. No entanto, a validade dessas hipóteses depende da verificação cuidadosa por meio de testes e evidências.\\n\\n\", \"1 Tempos atrás (2013), com humor, um “surto de honestidade” entre os cientistas percorreu o twitter com a hashtag #overlyhonestmethods. Naquela semana os cientistas fugiram da impessoalidade e frieza dos artigos científicos e foram honestos com o público expondo como realmente suas descobertas aconteceram.\\n\\n\\n  \\n    \\n      Photo by British Library on Unsplash Photo by British Library on Unsplash &#8617;\\n    \\n  \\n\\n\"]",
    "categories" : [["eletronica"],["eletronica"],["aplicadas"],["python"],["aplicadas"],["aplicadas"],["aplicadas"],["eletrônica"],["eletrônica"],["ciências"],["ciências"]],
    "featured" : [true,true,true,true,true,true,true,true,true,false,false],
    "tags" : [["música eletrônica","sintetizadores"],["obd2"],["programação linear","otimização","pesquisa operacional"],["AI","ChatGPT","python"],["python","otimização","CVXPY"],["matemática","probabilidade"],["matemática","estatística"],["eletrônica","python","circuitos elétricos"],["eletrônica","processamento de sinais","filtros"],["filosofia","ciências"],["filosofia","ciências"]]
}